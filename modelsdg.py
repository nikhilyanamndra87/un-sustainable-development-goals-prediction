# -*- coding: utf-8 -*-
"""Untitled42.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1USSYnyz4KOdmad5R75X2Kc5CO052YSxw
"""

pip  install streamlit

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import string
import re
import pickle

#importing required libraries from nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.tokenize import sent_tokenize, word_tokenize 

#importing streamlit
import streamlit as st

#importing required libraries from sklearn
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.feature_extraction.text import TfidfTransformer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import SGDClassifier
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score, classification_report
import spacy
import nltk
from typing import List
import numpy as np
import pandas as pd
import plotly.express as px
from tqdm import tqdm
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.feature_selection import SelectKBest, chi2, f_classif
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, top_k_accuracy_score, f1_score
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.ensemble import GradientBoostingClassifier
import re
nltk.download('stopwords')
nltk.download('punkt')
from nltk.tokenize import word_tokenize
sns.set_theme()

data = pd.read_csv('https://zenodo.org/record/5550238/files/osdg-community-dataset-v21-09-30.csv?download=1',header = 0, index_col = 0)

data.shape
data = data.query('agreement >= .6 and labels_positive > labels_negative').copy()
data.shape

import plotly.io as pio 
pio.templates.default = 'plotly_white'

spacy.prefer_gpu()
nlp = spacy.load('en_core_web_sm', disable = ['ner'])

print('SpaCy version:', spacy.__version__)

def preprocess_spacy(alpha: List[str]) -> List[str]:
    """
    Preprocess text input using spaCy.
    
    Parameters
    ----------
    alpha: List[str]
        a text corpus.
    
    Returns
    -------
    doc: List[str]
        a cleaned version of the original text corpus.
    """
    docs = list()
    
    for doc in tqdm(nlp.pipe(alpha, batch_size = 128)):
        tokens = list()
        for token in doc:
            if token.pos_ in ['NOUN', 'VERB', 'ADJ']:
                tokens.append(token.lemma_)
        docs.append(' '.join(tokens))
        
    return docs

data['docs'] = preprocess_spacy(data['text'].values)
data.shape
display(data.head())

x_train, x_test, y_train, y_test = train_test_split(
    data['docs'].values, 
    data['sdg'].values, 
    test_size = .3,
    random_state = 42
)

pipe = Pipeline([
    ('vectoriser', TfidfVectorizer(
        ngram_range = (1, 2),
        max_df = 0.75,
        min_df = 2,
        max_features = 100_000
    )),
    ('selector', SelectKBest(f_classif, k = 5_000)),
    ('clf', LogisticRegression(
        penalty = 'l2',
        C = .9,
        multi_class = 'multinomial',
        class_weight = 'balanced',
        random_state = 42,
        solver = 'newton-cg',
        max_iter = 100
    ))
])

classifier = pipe.fit(x_train, y_train)

y_pred = classifier.predict(x_test)
svm_acc = accuracy_score(y_pred, y_test)
print("Accuracy: ",svm_acc)

sampleText=["We are also focussing on scaling our adjacent businesses. In Services and Solutions, we are restructuring our distribution channels to cater to different segments while enhancing our manufacturing and execution capability. As we are seeing an urgent need for enhancing health infrastructure in the country, our Nest-In prefabricated product is now being deployed in providing COVID-19 isolation centres and for expansion of COVID-19 bed capacity across the country. We are therefore scaling up capacity and capability in this space. In the New Materials business, we are investing in creating a robust new product funnel while building strategic relationships. Sustainability and climate continues to be core focus areas in our strategy and business operations. We will continue to reduce our carbon emission footprint through process innovation and operational efficiency improvements."]
def probCal(sampleText):
	probs = classifier.predict_proba(sampleText)
	return probs

with open('model_pickle', 'wb') as files:
    pickle.dump(classifier, files)



